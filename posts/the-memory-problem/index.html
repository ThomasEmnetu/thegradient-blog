<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multi-Agent Systems & Their Tendency to Develop Alzheimers | Gradient.</title>
<meta name=keywords content="AI,Research,Engineering,Opinion"><meta name=description content="TLDR:
Everyone is excited about multi-agent swarms. Few are talking about the actual bottleneck. This post is about that bottleneck: memory.
We now have cheap inference, capable models, and production-ready orchestration frameworks. But when you scale to 10+ agents working in parallel across days of iteration, the coordination breaks down, not because the models are dumb, but because nobody has solved how agents store, share, and retrieve context at scale."><meta name=author content="Thomas Emnetu"><link rel=canonical href=https://thegradient.ink/posts/the-memory-problem/><link crossorigin=anonymous href=/assets/css/stylesheet.0e0a4a7e262f784d10323d22de959ebc81f1bc53e002a018f98d9062377139ac.css integrity="sha256-DgpKfiYveE0QMj0i3pWevIHxvFPgAqAY+Y2QYjdxOaw=" rel="preload stylesheet" as=style><link rel=icon href=https://thegradient.ink/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://thegradient.ink/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://thegradient.ink/favicon-32x32.png><link rel=apple-touch-icon href=https://thegradient.ink/apple-touch-icon.png><link rel=mask-icon href=https://thegradient.ink/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://thegradient.ink/posts/the-memory-problem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script data-goatcounter=https://thegradient.goatcounter.com/count async src=//gc.zgo.at/count.js></script><meta property="og:url" content="https://thegradient.ink/posts/the-memory-problem/"><meta property="og:site_name" content="Gradient."><meta property="og:title" content="Multi-Agent Systems & Their Tendency to Develop Alzheimers"><meta property="og:description" content="TLDR:
Everyone is excited about multi-agent swarms. Few are talking about the actual bottleneck. This post is about that bottleneck: memory.
We now have cheap inference, capable models, and production-ready orchestration frameworks. But when you scale to 10+ agents working in parallel across days of iteration, the coordination breaks down, not because the models are dumb, but because nobody has solved how agents store, share, and retrieve context at scale."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-18T00:01:00-07:00"><meta property="article:modified_time" content="2026-02-18T00:01:00-07:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="Research"><meta property="article:tag" content="Engineering"><meta property="article:tag" content="Opinion"><meta property="og:image" content="https://thegradient.ink/images/gradient-og.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://thegradient.ink/images/gradient-og.png"><meta name=twitter:title content="Multi-Agent Systems & Their Tendency to Develop Alzheimers"><meta name=twitter:description content="TLDR:
Everyone is excited about multi-agent swarms. Few are talking about the actual bottleneck. This post is about that bottleneck: memory.
We now have cheap inference, capable models, and production-ready orchestration frameworks. But when you scale to 10+ agents working in parallel across days of iteration, the coordination breaks down, not because the models are dumb, but because nobody has solved how agents store, share, and retrieve context at scale."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://thegradient.ink/posts/"},{"@type":"ListItem","position":2,"name":"Multi-Agent Systems \u0026 Their Tendency to Develop Alzheimers","item":"https://thegradient.ink/posts/the-memory-problem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multi-Agent Systems \u0026 Their Tendency to Develop Alzheimers","name":"Multi-Agent Systems \u0026 Their Tendency to Develop Alzheimers","description":"TLDR:\nEveryone is excited about multi-agent swarms. Few are talking about the actual bottleneck. This post is about that bottleneck: memory.\nWe now have cheap inference, capable models, and production-ready orchestration frameworks. But when you scale to 10+ agents working in parallel across days of iteration, the coordination breaks down, not because the models are dumb, but because nobody has solved how agents store, share, and retrieve context at scale.","keywords":["AI","Research","Engineering","Opinion"],"articleBody":"TLDR:\nEveryone is excited about multi-agent swarms. Few are talking about the actual bottleneck. This post is about that bottleneck: memory.\nWe now have cheap inference, capable models, and production-ready orchestration frameworks. But when you scale to 10+ agents working in parallel across days of iteration, the coordination breaks down, not because the models are dumb, but because nobody has solved how agents store, share, and retrieve context at scale.\nWhy we’re suddenly seeing multi-agent systems everywhere Before we get into the problem, we need to understand why this is even possible now. Long story short; inference got cheap. Like insanely cheap.\nMay 2024 Input 1M Output 1M Feb 2026 Input 1M Output 1M GPT-4 $30 $60 GPT-5.2 $1.75 $14 GPT-3.5-turbo $0.50 $1.50 GPT-5-mini $0.25 $2 That’s an 80%+ drop in cost for frontier intelligence in under 2 years.\nThis is why multi-agent systems went from research concept to production reality. When reasoning is this cheap, you can afford to run multiple agents in parallel without burning through budgets.\nBut cheap tokens created a new problem.\nThe hidden cost of context windows Multi-agent systems blow through context windows fast.\nEven with just 5 agents working in parallel, each maintaining their own plan files, logs, and understanding of the codebase, the token count explodes. Now imagine 15.\nAnthropic’s research on their C compiler project makes this concrete: 16 parallel agents consumed 2 billion input tokens across 2,000 Claude Code sessions (~2 weeks straight). That’s $20,000 in API costs for a single project.\nAnd this wasn’t some grandiose enterprise system. It was one compiler.\nMost modern LLMs have a ~200k token context window limit before performance degrades significantly. Some have 1M context window, which sounds like a lot, but larger context windows don’t solve the problem. Models with tighter windows tend to be more accurate and effective at complex agentic tasks precisely because context rot is real. When agents have to sift through hundreds of thousands of tokens before each action, they get lazy. Just like any human would.\nFor production systems, the economics compound fast. A company running 20+ agents across different workflows could easily be looking at $100k+ in monthly token costs if memory isn’t optimized.\nCheap inference made multi-agent systems possible. Bloated context is quietly making them expensive again.\nHow multi-agent systems actually work Let me show you the simple version first.\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Agent 1 │ → │ Agent 2 │ → │ Agent 3 │ │ (PM) │ │ (Backend) │ │ (Frontend) │ └─────────────┘ └─────────────┘ └─────────────┘ ↓ ↓ ↓ plan/*.md server/src/ client/src/ Three agents. Linear workflow. Each one writes plan files and artifacts that the following agents can consume in tandem.\nNow here’s what happens when you scale this to a relatively modest production:\n┌──────────────────┐ │ PM Agent │ │ Writes all specs │ └────────┬─────────┘ │ ┌────────────────────┼────────────────────┐ │ │ │ ▼ ▼ ▼ ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ │ Phase 4A │ │ Phase 4B │ │ Phase 4C │ │ Database │ │ AI/LLM │ │ DevOps │ └───────┬───────┘ └───────┬───────┘ └───────┬───────┘ │ │ │ │ ┌─────────┘ │ ▼ ▼ │ ┌─────────────────────────┐ │ │ Phase 5A │ │ │ API Agent │ │ │ (Prisma + LLM logic) │ │ └───────────┬─────────────┘ │ │ │ │ ┌───────────────────────┘ ▼ ▼ ┌─────────────────────────┐ │ Phase 5B │ │ Worker Agents │ └───────────┬─────────────┘ │ ├──────────────────────┐ ▼ ▼ ┌───────────────────┐ ┌───────────────────┐ │ Phase 6 │ │ Phase 7 │ │ Frontend │ │ Deploy │ └─────────┬─────────┘ └─────────┬─────────┘ │ │ └────────────┬───────────┘ ▼ ┌───────────────────┐ │ Phase 8 │ │ QA Agent │ └─────────┬─────────┘ │ ┌────────────┴────────────┐ │ │ ✅ PASS ❌ FAIL │ │ PRODUCTION Route to owner COMPLETE (loops back) Suddenly you have 8+ phases, 15+ specialized agents, complex dependencies, merge conflicts, and poor code.\nSo the billion dollar question is: how do you keep them all coordinated? Especially in large scale legacy codebases.\nThe memory architecture problem Each agent needs to know:\nWhat other agents have done What’s currently in progress What to do at this moment and beyond What conflicts to be wary of What the current state of the codebase is The current common solution is markdown files.\nComponent Purpose plan/phase-1.md PM agent’s research and specs plan/phase-2.md Backend agent’s implementation plan plan/phase-3.md Frontend agent’s UI plan plan/agent-logs/ Timestamped task completion records plan/overview-memory.md Central coordination document Every agent reads the overview file. Updates it. Other agents see those updates.\nIn theory, they stay synchronized. But in practice, this falls apart at scale.\nWhere it breaks MongoDB’s research on multi-agent memory found that LLM performance systematically degrades when context exceeds certain thresholds.\nThe models condense information. They optimize. But they don’t know what’s actually important to keep.\nAnthropic saw this firsthand. When building their multi-agent research system, early versions had agents “spawning 50 subagents for simple queries, scouring the web endlessly for nonexistent sources, and distracting each other with excessive updates.”\nThis wasn’t a model problem. It was a memory architecture problem.\nWhen you’re coordinating 15+ agents across different phases, each reading \u0026 writing .md plan files, logs, and context, the degradation compounds exponentially. The models lose track. They repeat work. They conflict with each other.\nWhat some are trying OpenClaw built something interesting: a markdown “source of truth” system that attempts to mimic human cognitive recall.\nIt works like this:\nDaily logs - timestamped record of what happened Entity indexes - key concepts, files, decisions Reflection mechanisms - what worked, what didn’t Retrieval strategies - lexical (exact terms), temporal (time-based), entity (topic-based) It makes sense logically. As humans, we don’t remember what we ate for dinner the other day, let alone every detail from a year ago. But you do remember the important entities. The patterns. The outcomes.\nBut relying on purely markdown file system for memory seems juvenile for large codebases and tasks.\nGithub recently released their solution to this problem. They developed a novel approach that stores memories with citations (i.e. references to specific code locations that support each fact) rather than a strictly plain text markdown system.\nHow Github Copilot agents store learnings worth remembering as they carry out their tasks Others are experimenting with:\nSQL databases with indexed memory files Embedding-based retrieval (RAG for memory) Hybrid systems combining daily logs with entity graphs Everyone’s trying something. But the enterprise-grade solution gap still exists.\nThe token economics of memory Here’s the brutal reality:\nWhen Anthropic shipped their multi-agent research system, they found that “agents typically use about 4× more tokens than chat interactions, and multi-agent systems use about 15× more tokens than chats.”\nBut they also reportedly discovered that when memory and coordination work together, multi-agent systems outperformed single-agent systems by 90%. And that was with Claude Opus 4…\nThis truly signals a categorical shift in capability, and this trend is likely to continue due to scaling laws.\nThe problem is no longer that multi-agent systems don’t work effectively, but that we don’t have the memory infrastructure to support them efficiently.\nWhat needs to be solved The bottleneck changed from individual intelligence to coordination and resource allocation.\nThe models are more than capable. Opus 4.5 and 4.6 can code extremely well, reason, and plan at high levels.\nThe question is: how do we architect memory storage and recall that scales?\nNot for 1 session or 1 task, but across:\nMultiple agents working in parallel Different branches and contexts Days or weeks of iteration Thousands of file changes Millions of tokens of accumulated context How do you structure memory so agents can coordinate without degrading? How do you build retrieval systems that know what to keep and when to forget? How do you create indexes that survive 10,000+ token interactions?\nThese are the questions that actually matter.\nMy perspective on this I’m still optimistic about multi-agent systems.\nI’ve seen what they can do when they work. The rapid deployments, autonomous bug fixes, ability to delegate entire workflows in a closed feedback loop.\nBut I’m frustrated by the gap between the hype and the reality.\nPeople are understandably excited about the progression of autonomous agents and Multi-Agent workflows. I am too. But it’s important to not get caught up in the exuberance when the industry is still facing a significant blocker.\nUntil the memory problem is cracked, we’re simply scaling broken systems and burning tokens.\nThe foundational models are really good. The bottleneck is how you architect an agentic system that’s actually powerful.\nThat’s the work. Orchestration. Not waiting for better models. Not hyping up the next demo you see on twitter. But rather building a memory layer that makes coordination possible at scale.\nI personally believe the key lies in mimicking how humans encode, store, and retrieve information.\nTo me this means leveraging a 2-tier architecture:\nTraining a lightweight, specialized memory management model that handles the constant layer of:\n“what’s happening, what do I need to know”\nThen this lightweight model could then feed clean, compressed context to the main agent model that can dedicate it’s full context window to actual execution.\nRather than each agent spending 60-80% of it’s context just reorientating “where are we, what’s my role, what are the dependencies” before they even start thinking about the task.\nAcknowledgements Special thanks to Ahmed Ghaddah for sharing your insights which inspired this post.\n","wordCount":"1534","inLanguage":"en","image":"https://thegradient.ink/images/gradient-og.png","datePublished":"2026-02-18T00:01:00-07:00","dateModified":"2026-02-18T00:01:00-07:00","author":[{"@type":"Person","name":"Thomas Emnetu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://thegradient.ink/posts/the-memory-problem/"},"publisher":{"@type":"Organization","name":"Gradient.","logo":{"@type":"ImageObject","url":"https://thegradient.ink/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://thegradient.ink/ accesskey=h title="Gradient. (Alt + H)"><img src=https://thegradient.ink/images/temnetu-logo.svg alt aria-label=logo height=55>Gradient.</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://thegradient.ink/>Home</a>&nbsp;»&nbsp;<a href=https://thegradient.ink/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Multi-Agent Systems & Their Tendency to Develop Alzheimers</h1><div class=post-meta><span title='2026-02-18 00:01:00 -0700 -0700'>February 18, 2026</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Thomas Emnetu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#why-were-suddenly-seeing-multi-agent-systems-everywhere aria-label="Why we&rsquo;re suddenly seeing multi-agent systems everywhere">Why we&rsquo;re suddenly seeing multi-agent systems everywhere</a></li><li><a href=#the-hidden-cost-of-context-windows aria-label="The hidden cost of context windows">The hidden cost of context windows</a></li><li><a href=#how-multi-agent-systems-actually-work aria-label="How multi-agent systems actually work">How multi-agent systems actually work</a></li><li><a href=#the-memory-architecture-problem aria-label="The memory architecture problem">The memory architecture problem</a></li><li><a href=#where-it-breaks aria-label="Where it breaks">Where it breaks</a></li><li><a href=#what-some-are-trying aria-label="What some are trying">What some are trying</a></li><li><a href=#the-token-economics-of-memory aria-label="The token economics of memory">The token economics of memory</a></li><li><a href=#what-needs-to-be-solved aria-label="What needs to be solved">What needs to be solved</a></li><li><a href=#my-perspective-on-this aria-label="My perspective on this">My perspective on this</a><ul><li><a href=#acknowledgements aria-label=Acknowledgements>Acknowledgements</a></li></ul></li></ul></div></details></div><div class=post-content><p>TLDR:</p><p><em>Everyone is excited about multi-agent swarms. Few are talking about the actual bottleneck. This post is about that bottleneck: memory.</em></p><p><em>We now have cheap inference, capable models, and production-ready orchestration frameworks. But when you scale to 10+ agents working in parallel across days of iteration, the coordination breaks down, not because the models are dumb, but because nobody has solved how agents store, share, and retrieve context at scale.</em></p><h2 id=why-were-suddenly-seeing-multi-agent-systems-everywhere>Why we&rsquo;re suddenly seeing multi-agent systems everywhere<a hidden class=anchor aria-hidden=true href=#why-were-suddenly-seeing-multi-agent-systems-everywhere>#</a></h2><p>Before we get into the problem, we need to understand why this is even possible now. Long story short; inference got cheap. Like insanely cheap.</p><table><thead><tr><th>May 2024</th><th>Input 1M</th><th>Output 1M</th><th>Feb 2026</th><th>Input 1M</th><th>Output 1M</th></tr></thead><tbody><tr><td>GPT-4</td><td>$30</td><td>$60</td><td>GPT-5.2</td><td>$1.75</td><td>$14</td></tr><tr><td>GPT-3.5-turbo</td><td>$0.50</td><td>$1.50</td><td>GPT-5-mini</td><td>$0.25</td><td>$2</td></tr></tbody></table><p>That&rsquo;s an <strong>80%+ drop in cost</strong> for frontier intelligence in under 2 years.</p><p>This is why multi-agent systems went from research concept to production reality. When reasoning is this cheap, you can afford to run multiple agents in parallel without burning through budgets.</p><p>But cheap tokens created a new problem.</p><h2 id=the-hidden-cost-of-context-windows>The hidden cost of context windows<a hidden class=anchor aria-hidden=true href=#the-hidden-cost-of-context-windows>#</a></h2><p>Multi-agent systems blow through context windows fast.</p><p>Even with just 5 agents working in parallel, each maintaining their own plan files, logs, and understanding of the codebase, the token count explodes. Now imagine 15.</p><p><a href=https://www.anthropic.com/engineering/building-c-compiler>Anthropic&rsquo;s research on their C compiler project</a> makes this concrete: <strong>16 parallel agents consumed 2 billion input tokens</strong> across 2,000 Claude Code sessions (~2 weeks straight). That&rsquo;s $20,000 in API costs for a single project.</p><p>And this wasn&rsquo;t some grandiose enterprise system. It was one compiler.</p><p>Most modern LLMs have a ~200k token context window limit before performance degrades significantly. Some have 1M context window, which sounds like a lot, but larger context windows don&rsquo;t solve the problem. Models with tighter windows tend to be more accurate and effective at complex agentic tasks precisely because context rot is real. When agents have to sift through hundreds of thousands of tokens before each action, they get lazy. Just like any human would.</p><p>For production systems, the economics compound fast. A company running 20+ agents across different workflows could easily be looking at $100k+ in monthly token costs if memory isn&rsquo;t optimized.</p><p>Cheap inference made multi-agent systems possible. Bloated context is quietly making them expensive again.</p><h2 id=how-multi-agent-systems-actually-work>How multi-agent systems actually work<a hidden class=anchor aria-hidden=true href=#how-multi-agent-systems-actually-work>#</a></h2><p>Let me show you the simple version first.</p><pre tabindex=0><code>┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  Agent 1    │ →  │  Agent 2    │ →  │  Agent 3    │
│    (PM)     │    │  (Backend)  │    │ (Frontend)  │
└─────────────┘    └─────────────┘    └─────────────┘
       ↓                  ↓                  ↓
    plan/*.md         server/src/        client/src/
</code></pre><p>Three agents. Linear workflow. Each one writes plan files and artifacts that the following agents can consume in tandem.</p><p>Now here&rsquo;s what happens when you scale this to a relatively modest production:</p><pre tabindex=0><code>                    ┌──────────────────┐
                    │    PM Agent      │
                    │ Writes all specs │
                    └────────┬─────────┘
                             │
        ┌────────────────────┼────────────────────┐
        │                    │                    │
        ▼                    ▼                    ▼
┌───────────────┐   ┌───────────────┐   ┌───────────────┐
│  Phase 4A     │   │  Phase 4B     │   │  Phase 4C     │
│  Database     │   │  AI/LLM       │   │  DevOps       │
└───────┬───────┘   └───────┬───────┘   └───────┬───────┘
        │                   │                   │
        │         ┌─────────┘                   │
        ▼         ▼                             │
┌─────────────────────────┐                     │
│      Phase 5A           │                     │
│      API Agent          │                     │
│  (Prisma + LLM logic)   │                     │
└───────────┬─────────────┘                     │
            │                                   │
            │           ┌───────────────────────┘
            ▼           ▼
┌─────────────────────────┐
│      Phase 5B           │
│   Worker Agents         │
└───────────┬─────────────┘
            │
            ├──────────────────────┐
            ▼                      ▼
┌───────────────────┐    ┌───────────────────┐
│    Phase 6        │    │    Phase 7        │
│   Frontend        │    │   Deploy          │
└─────────┬─────────┘    └─────────┬─────────┘
          │                        │
          └────────────┬───────────┘
                       ▼
            ┌───────────────────┐
            │    Phase 8        │
            │    QA Agent       │
            └─────────┬─────────┘
                      │
         ┌────────────┴────────────┐
         │                         │
    ✅ PASS                    ❌ FAIL
         │                         │
    PRODUCTION              Route to owner
     COMPLETE               (loops back)
</code></pre><p>Suddenly you have 8+ phases, 15+ specialized agents, complex dependencies, merge conflicts, and poor code.</p><p>So the billion dollar question is: <strong>how do you keep them all coordinated?</strong> Especially in large scale legacy codebases.</p><h2 id=the-memory-architecture-problem>The memory architecture problem<a hidden class=anchor aria-hidden=true href=#the-memory-architecture-problem>#</a></h2><p>Each agent needs to know:</p><ul><li>What other agents have done</li><li>What&rsquo;s currently in progress</li><li>What to do at this moment and beyond</li><li>What conflicts to be wary of</li><li>What the current state of the codebase is</li></ul><p>The current common solution is markdown files.</p><table><thead><tr><th>Component</th><th>Purpose</th></tr></thead><tbody><tr><td><code>plan/phase-1.md</code></td><td>PM agent&rsquo;s research and specs</td></tr><tr><td><code>plan/phase-2.md</code></td><td>Backend agent&rsquo;s implementation plan</td></tr><tr><td><code>plan/phase-3.md</code></td><td>Frontend agent&rsquo;s UI plan</td></tr><tr><td><code>plan/agent-logs/</code></td><td>Timestamped task completion records</td></tr><tr><td><code>plan/overview-memory.md</code></td><td>Central coordination document</td></tr></tbody></table><p>Every agent reads the overview file. Updates it. Other agents see those updates.</p><p>In theory, they stay synchronized. But in practice, this falls apart at scale.</p><h2 id=where-it-breaks>Where it breaks<a hidden class=anchor aria-hidden=true href=#where-it-breaks>#</a></h2><p><a href=https://www.mongodb.com/company/blog/technical/why-multi-agent-systems-need-memory-engineering>MongoDB&rsquo;s research on multi-agent memory</a> found that <strong>LLM performance systematically degrades when context exceeds certain thresholds.</strong></p><p>The models condense information. They optimize. But they don&rsquo;t know what&rsquo;s actually important to keep.</p><p>Anthropic saw this firsthand. When building their multi-agent research system, early versions had agents <a href=https://www.anthropic.com/engineering/multi-agent-research-system>&ldquo;spawning 50 subagents for simple queries, scouring the web endlessly for nonexistent sources, and distracting each other with excessive updates.&rdquo;</a></p><p>This wasn&rsquo;t a model problem. It was a memory architecture problem.</p><p>When you&rsquo;re coordinating 15+ agents across different phases, each reading & writing .md plan files, logs, and context, the degradation compounds exponentially. The models lose track. They repeat work. They conflict with each other.</p><h2 id=what-some-are-trying>What some are trying<a hidden class=anchor aria-hidden=true href=#what-some-are-trying>#</a></h2><p><a href=https://docs.openclaw.ai/concepts/memory>OpenClaw built something interesting: a markdown &ldquo;source of truth&rdquo;</a> system that attempts to mimic human cognitive recall.</p><p>It works like this:</p><ol><li><strong>Daily logs</strong> - timestamped record of what happened</li><li><strong>Entity indexes</strong> - key concepts, files, decisions</li><li><strong>Reflection mechanisms</strong> - what worked, what didn&rsquo;t</li><li><strong>Retrieval strategies</strong> - lexical (exact terms), temporal (time-based), entity (topic-based)</li></ol><p>It makes sense logically. As humans, we don&rsquo;t remember what we ate for dinner the other day, let alone every detail from a year ago. But you do remember the important entities. The patterns. The outcomes.</p><p>But relying on purely markdown file system for memory seems juvenile for large codebases and tasks.</p><p><a href=https://github.blog/ai-and-ml/github-copilot/building-an-agentic-memory-system-for-github-copilot/>Github recently released their solution to this problem.</a> They developed a novel approach that stores memories with citations (i.e. references to specific code locations that support each fact) rather than a strictly plain text markdown system.</p><figure><img src="/images/github-memory generation solution.jpeg" alt="Github memory solution"><figcaption>How Github Copilot agents store learnings worth remembering as they carry out their tasks</figcaption></figure><p>Others are experimenting with:</p><ul><li>SQL databases with indexed memory files</li><li>Embedding-based retrieval (RAG for memory)</li><li>Hybrid systems combining daily logs with entity graphs</li></ul><p>Everyone&rsquo;s trying something. But the enterprise-grade solution gap still exists.</p><h2 id=the-token-economics-of-memory>The token economics of memory<a hidden class=anchor aria-hidden=true href=#the-token-economics-of-memory>#</a></h2><p>Here&rsquo;s the brutal reality:</p><p>When Anthropic shipped their multi-agent research system, they found that <a href=https://www.anthropic.com/engineering/multi-agent-research-system>&ldquo;agents typically use about 4× more tokens than chat interactions, and multi-agent systems use about 15× more tokens than chats.&rdquo;</a></p><p>But they also reportedly discovered that when memory and coordination work together, multi-agent systems outperformed single-agent systems by 90%. And that was with Claude Opus 4&mldr;</p><p>This truly signals a categorical shift in capability, and this trend is likely to continue due to scaling laws.</p><p>The problem is no longer that multi-agent systems don&rsquo;t work effectively, but that we don&rsquo;t have the memory infrastructure to support them efficiently.</p><h2 id=what-needs-to-be-solved>What needs to be solved<a hidden class=anchor aria-hidden=true href=#what-needs-to-be-solved>#</a></h2><p>The bottleneck changed from individual intelligence to coordination and resource allocation.</p><p>The models are more than capable. Opus 4.5 and 4.6 can code extremely well, reason, and plan at high levels.</p><p>The question is: how do we architect memory storage and recall that scales?</p><p>Not for 1 session or 1 task, but across:</p><ul><li>Multiple agents working in parallel</li><li>Different branches and contexts</li><li>Days or weeks of iteration</li><li>Thousands of file changes</li><li>Millions of tokens of accumulated context</li></ul><p>How do you structure memory so agents can coordinate without degrading? How do you build retrieval systems that know what to keep and when to forget? How do you create indexes that survive 10,000+ token interactions?</p><p>These are the questions that actually matter.</p><h2 id=my-perspective-on-this>My perspective on this<a hidden class=anchor aria-hidden=true href=#my-perspective-on-this>#</a></h2><p>I&rsquo;m still optimistic about multi-agent systems.</p><p>I&rsquo;ve seen what they can do when they work. The rapid deployments, autonomous bug fixes, ability to delegate entire workflows in a closed feedback loop.</p><p>But I&rsquo;m frustrated by the gap between the hype and the reality.</p><p>People are understandably excited about the progression of autonomous agents and Multi-Agent workflows. I am too. But it&rsquo;s important to not get caught up in the exuberance when the industry is still facing a significant blocker.</p><p>Until the memory problem is cracked, we&rsquo;re simply scaling broken systems and burning tokens.</p><p>The foundational models are really good. The bottleneck is how you architect an agentic system that&rsquo;s actually powerful.</p><p>That&rsquo;s the work. Orchestration. Not waiting for better models. Not hyping up the next demo you see on twitter. But rather building a memory layer that makes coordination possible at scale.</p><p>I personally believe the key lies in mimicking how humans encode, store, and retrieve information.</p><p>To me this means leveraging a 2-tier architecture:</p><p>Training a lightweight, specialized memory management model that handles the constant layer of:</p><blockquote><p>&ldquo;what&rsquo;s happening, what do I need to know&rdquo;</p></blockquote><p>Then this lightweight model could then feed clean, compressed context to the main agent model that can dedicate it&rsquo;s full context window to actual execution.</p><p>Rather than each agent spending 60-80% of it&rsquo;s context just reorientating &ldquo;where are we, what&rsquo;s my role, what are the dependencies&rdquo; before they even start thinking about the task.</p><h3 id=acknowledgements>Acknowledgements<a hidden class=anchor aria-hidden=true href=#acknowledgements>#</a></h3><p>Special thanks to Ahmed Ghaddah for sharing your insights which inspired this post.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://thegradient.ink/tags/ai/>AI</a></li><li><a href=https://thegradient.ink/tags/research/>Research</a></li><li><a href=https://thegradient.ink/tags/engineering/>Engineering</a></li><li><a href=https://thegradient.ink/tags/opinion/>Opinion</a></li></ul><nav class=paginav><a class=next href=https://thegradient.ink/posts/embracing-ai-agents/><span class=title>Next »</span><br><span>I’ve Been Drinking the AI Kool-Aid…and I Love It</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://thegradient.ink/>Gradient.</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>